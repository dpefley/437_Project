import urllib2
import json
import time
from unicodedata import normalize
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# BIG PROBLEM: CODE STOPS BECAUSE FIREFOX'S SCROLL BAR COVERS THE LINK; CHROME DOES NOT

# data = {
#     all_courses: {
#         'CSE 247': {
#             code: 'CSE 247',
#             department: 'E81'
#             description: '<<stuff>>',
#             name: 'Data Structures and Algorithms',
#             prerequisites: [
#                 'CSE 240'
#             ],
#             school: 'Engineering and Applied Science'
#         }
#         .
#         .
#         .
#     },
#     departments: {
#         'E62': ...,
#         'E81': Computer Science and Engineering,
#         .
#         .
#         .
#     },
#     majors: {
#         //gonna have to look at additional pages to parse: check out h2 tags?
#         'Bachelor of Science in Computer Science': {
#             requirements: {
#                 'CSE 131',
#                 'CSE 132',
#                 electives_1: {
#                     required_units: 3.0,
#                     options: {
#                         //list A courses here
#                     }
#                 },
#                 .
#                 .
#                 .
#             }
#         }
#     },
#     minors: {
#         //gonna have to look at additional pages to parse
#         //similar format to majors
#     },
#     schools: {
#         'Engineering and Applied Science': {
#             departments: {
#                 'E81': {
#                     courses: {
#                         {
#                             code: 'CSE 247',
#                             description: '<<stuff>>',
#                             name: 'Data Structures and Algorithms',
#                             prerequisites: [
#                                 'CSE 240'
#                             ]
#                         }
#                     }
#                 }
#                 .
#                 .
#                 .
#             }
#         }
#         .
#         .
#         .
#     }
# }

schools = []
departments = []
courses = []
data = []

base_semester_link = 'Body_hlSemester'
more_semesters_link = 'Body_hlMoreSemesters'
base_more_semesters_link = 'Body_repMoreSems_hlMore_'
base_school_link = 'Body_repSchools_lnkSchool_'
base_dept_link = 'Body_dlDepartments_lnkDept_'
expanded_link = 'vwExpanded'

quote_page = 'https://acadinfo.wustl.edu/CourseListings/Semester/Listing.aspx'

def getDescription(divTag):
    course_description = ""
    fourth_table = divTag.find_next('table').find_next('table').find_next('table').find_next('table')
    description_a = fourth_table.find_next('a').find_next('a').getText()
    course_description = description_a              
    return course_description

def getPrerequisites(divTag):
    course_prereq = "**No Prerequisites Found**"
    description = getDescription(divTag)
    lower_desc = description.lower()
    pattern = "prereq" 
    prereq_index = lower_desc.find(pattern)
    if prereq_index > 0:
        before,after = lower_desc.split(pattern) # Throws error: Too many values to unpack
        after_arr = after.split() 
        course_prereq = after_arr[1:]
        s = " "
        printer = s.join(course_prereq)
        return printer
    fourth_table = divTag.find_next('table').find_next('table').find_next('table').find_next('table')
    prereq_a = fourth_table.find_next('a').find_next('a').find_next('a').getText()
    lower_prereq_a = prereq_a.lower() 
    prereq_index = lower_prereq_a.find(pattern)
    if prereq_index > 0:
        before,after = lower_prereq_a.split(pattern)
        after_arr = after.split() 
        course_prereq = after_arr[1:]
        s = " "
        printer = s.join(course_prereq)
        return printer
    return course_prereq # Return the output

def getPageInfo(soup, school, department): #div[class*="listing-col-"]
    for div_tag in soup.select('div[class*="Crs"]'):
        basic_course_info = div_tag.find('table').find('table')

        count = 0
        course_code = ""
        course_dept = ""
        course_name = ""
        course_units = ""
        course_units_val = 0.0
        has_set_credits = True
        passed_equal_sign = False
        for a_tag in basic_course_info.find_all('a'):
            if (count == 0):
                course_code = a_tag.text
                course_code = normalize('NFKD', course_code) # gets rid of \xa0 and other non-unicode chars

                parts = a_tag.text.split()
                isFirst = True
                for part in parts:
                    if isFirst:
                        course_dept = part
                    else:
                        course_code += part
                        course_code += " "
                course_code = course_code[:-1]
            elif (count == 1):
                course_name = a_tag.text
            elif (count == 2):
                course_units = a_tag.text
                parts = course_units.split()
                isFirst = True
                for part in parts:
                    if isFirst:
                        if part == "Var.":
                            has_set_credits = False
                        else:
                            course_units_val = float(part)
                        isFirst = False
                    elif has_set_credits == False:
                        if passed_equal_sign:
                            course_units_val = float(part[:-1])
                        elif part == "=":
                            passed_equal_sign = True
            count += 1
        a_course = {
            'code': course_code,
            'department': department,
            'description': getDescription(div_tag),
            'has_set_credits': has_set_credits,
            'name': course_name,
            "prerequisites": "TBA",
            'school': school,
            'units': course_units_val
        }
        courses.append(a_course)

def main(): #BIG PROBLEM: KEEPS GETTING STUCK AND ADDING THE SAME DEPARTMENT'S COURSES REPEATEDLY
    driver = webdriver.Chrome()
    driver.get(quote_page)
    
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')
    
    school_div = ""
    for div_tag in soup.find_all('div', {"class": "Schools"}):
        school_div = div_tag
    for a_tag in school_div.find_all('a', {"class": "ControlLink"}):
        schools.append(a_tag['id'])
    
    for school in schools:
        driver.find_element_by_id(school).click()
        time.sleep(5)
        
        inner_soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        school_departments = []
        department_div = ""
        for div_tag in inner_soup.find_all('div', {"class": "Departments"}):
            department_div = div_tag
        if department_div != "":
            for a_tag in department_div.find_all('a', {"class": "ControlLink"}):
                school_departments.append(a_tag['id'])
        
        for department in school_departments:
            if department != 'Body_dlDepartments_lnkDept_0':
                driver.find_element_by_id(department).click()
                time.sleep(5)
                
                course_soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                this_school = course_soup.find(id = school)
                this_dept = course_soup.find(id = department)
                getPageInfo(course_soup, this_school.text, this_dept.text)
                print this_school.text,
                print this_dept.text
            
    print courses
    data = {
        "Courses": courses
    }
    with open('json_data.json', 'w') as f:
        json.dump(data, f)
    driver.quit()

main()